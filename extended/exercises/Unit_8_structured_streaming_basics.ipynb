{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will see how to implement the popular WordCount application against a streaming data source coming from a TCP socket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Review the code\n",
    "Start by reviewing the code of the app in exercises/Unit_8_structured_streaming-unbounded_table.py\n",
    "\n",
    "This is a complete streaming app with a input source and an output sink.\n",
    "\n",
    "## Step 2: Create a listening TCP socket\n",
    "You can create a listening TCP socket using netcat:\n",
    "\n",
    "    nc -l -k <port>\n",
    "    \n",
    "where <port> is the port where you want netcat to listen.\n",
    "\n",
    "You have to select a port that is not in use, so to avoid conflicts use the following:\n",
    "    \n",
    "    port = 10000 + <account_number>\n",
    "    \n",
    "where *account_number* is your account number, eg. if I am using curso801 I will use the port 10801, so I will run:\n",
    "    \n",
    "    nc -l -k 10801\n",
    "    \n",
    "Once that you know which port to use run the following commands:\n",
    "    \n",
    "    hostname\n",
    "    nc -l -k <port>\n",
    "    \n",
    "## Step 3: Run the app\n",
    "Using the values of *hostname* and *port* from the previous commands to run the app:\n",
    "    \n",
    "    module load anaconda3/2020.02\n",
    "    spark-submit --conf spark.dynamicAllocation.enabled=false --num-executors 2 Unit_8_structured_streaming-unbounded_table.py <hostname> <port>\n",
    "\n",
    "## Step 4: Test the different output modes\n",
    "Now run the app testing the different output modes: \"update\", \"append\" and \"complete\".\n",
    "    \n",
    "Provide some input and look at the output generated.\n",
    "    \n",
    "You will see that in this case \"complete\" mode is not supported because there are no streaming aggregations in our application so spark is not able to return all historical data because it is not keeping track of it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
