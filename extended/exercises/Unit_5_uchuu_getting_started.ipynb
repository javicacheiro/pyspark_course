{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "1. Reading Halos\n",
    "2. Processing Data\n",
    "3. Saving the results\n",
    "4. Beyond the basics\n",
    "4.1. Taking a reduced random sample\n",
    "4.2. Available redshits\n",
    "4.3. Inspecting the halos dataframe\n",
    "4.4. Getting a summary of basic statistics about a DataFrame in Spark\n",
    "4.5. Spark Tips\n",
    "4.6. Calculating the median\n",
    "4.7. Additional notes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, remember to import this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Halos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start loading all the halos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.03 ms, sys: 2.84 ms, total: 5.87 ms\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_halos = spark.read.parquet('/user/csiaafpr/RockstarExtendedParquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we proceed with our anlysis it is always important to **restrict them to the redshift we are interested in**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos = all_halos.where(col('redshift') == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The list of availabe redshifts can be found here:\n",
    "[UCHUU Snapshot Redshift correspondences](http://www.skiesanduniverses.org/resources/Uchuu_snapshot_redshift_scalefactor.txt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select several redshifts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos = all_halos.where((col('redshift') == 1.54) | (col('redshift') == 0.49))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even a range of redshits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos = all_halos.where((col('redshift') < 1.54) & (col('redshift') > 0.49))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with SQL you can also express the condition as a SQL predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos = all_halos.where('redshift > 0.10 and redshift < 0.50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select HOST halos for `Z=0` (redshift=0) in the Mvir range `cmass_min` - `cmass_max`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmass_min = 2.00e15\n",
    "cmass_max = 2.03e15\n",
    "\n",
    "hosts = all_halos.where((col('redshift') == 0.49)\n",
    "                    & (col('pid') == -1)\n",
    "                    & (col('Mvir') > cmass_min)\n",
    "                    & (col('Mvir') < cmass_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the most generic case, we can even restrict the halos selected indicating additional conditions they must fullfil and selecting just part of the columns of the dataframe, including additional computed ones and taking just a random sample of the halos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos = (all_halos.where((col('redshift') == 1.54)\n",
    "                    & (col('pid') == -1)\n",
    "                    & (col('Mvir') > 1.0e14)\n",
    "                    & (col('Mvir') < 1.3e14)\n",
    "                    & (col('Xoff')/col('Rvir') < 0.05)\n",
    "                    & (col('Spin') < 0.03))\n",
    "              .select('id', 'x', 'y', 'z', 'vx', 'vy', 'vz', 'Mvir', \n",
    "                      'Rvir', expr('Rvir/Rs_Klypin'))\n",
    "              .sample(0.08))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of of halos we have selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 ms, sys: 1.21 ms, total: 6.21 ms\n",
      "Wall time: 26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "halos.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show two of the halos in our selection (keep in mind that it will have to compute the additional columns so this will take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-------+-------+------+------+-------+--------+-------+------------------+\n",
      "|            id|                 x|      y|      z|    vx|    vy|     vz|    Mvir|   Rvir|(Rvir / Rs_Klypin)|\n",
      "+--------------+------------------+-------+-------+------+------+-------+--------+-------+------------------+\n",
      "| 4226292660622|2.8652900000000003|623.168|388.151|-41.45|-352.1|-256.26|1.013E14| 1140.7|  6.73086568361922|\n",
      "|81020308687922|           563.068|1758.17|1832.21|-13.99| 91.62| -99.75|1.135E14|1184.58|  6.28584467132214|\n",
      "+--------------+------------------+-------+-------+------+------+-------+--------+-------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 6.52 ms, sys: 1.64 ms, total: 8.16 ms\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "halos.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get inspiration about how to process the data and do some actual computations you can look at the Unit_5_computing_subhalo_clusters.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to save the resulting dataframe you use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos.write.parquet('halos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parquet is the recommended format to save data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can later load again the dataframe from the stored data simply executing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos = spark.read.parquet('halos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be stored in HDFS. \n",
    "\n",
    "You can download them to your HOME directory and from there transfer them by SCP to your PC (just keep in mind that the size of the resulting dataframe is reasonable for this):\n",
    "\n",
    "```\n",
    "hdfs dfs -get hosts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer, you can also save the results using less efficient formats like CSV or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos.write.csv('halos-csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "halos.write.json('halos-json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Beyond the basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a reduced random sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to take a reduced random sample of the data for experimenting before moving to all the data. For example in this case we will take 1% of the selected halos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = halos.sample(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available redshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correspondence between the snapshot number and the redshift can be found here:\n",
    "[UCHUU Snapshot Redshift correspondences](http://www.skiesanduniverses.org/resources/Uchuu_snapshot_redshift_scalefactor.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look a the list of redshifts currently available in the Hadoop cluster you can run (keep in mind that the list will be growing until we upload the 50 epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|redshift|\n",
      "+--------+\n",
      "|     0.0|\n",
      "|    3.93|\n",
      "|    1.54|\n",
      "|    4.27|\n",
      "|     0.7|\n",
      "|     2.3|\n",
      "|   0.022|\n",
      "|    9.47|\n",
      "|    2.03|\n",
      "|    3.61|\n",
      "|    1.65|\n",
      "|    7.76|\n",
      "|    0.63|\n",
      "|    2.78|\n",
      "|    3.31|\n",
      "|    0.19|\n",
      "|    1.22|\n",
      "|    4.63|\n",
      "|   11.51|\n",
      "|    2.46|\n",
      "|    0.49|\n",
      "|     1.9|\n",
      "|    0.14|\n",
      "|    5.73|\n",
      "|    7.02|\n",
      "|    0.86|\n",
      "|    0.78|\n",
      "|    0.56|\n",
      "|    2.95|\n",
      "|    6.34|\n",
      "|    1.32|\n",
      "|    0.36|\n",
      "|   13.96|\n",
      "|    0.94|\n",
      "|    1.12|\n",
      "|    1.03|\n",
      "|    0.25|\n",
      "|    1.77|\n",
      "|    0.43|\n",
      "|     0.3|\n",
      "|    8.58|\n",
      "|   12.69|\n",
      "|   10.44|\n",
      "|   0.093|\n",
      "|    5.16|\n",
      "|    3.13|\n",
      "|   0.045|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_halos.select('redshift').distinct().show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the halos dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the fields included in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A_x: double (nullable = true)\n",
      " |-- A_x_500c: double (nullable = true)\n",
      " |-- A_y: double (nullable = true)\n",
      " |-- A_y_500c: double (nullable = true)\n",
      " |-- A_z: double (nullable = true)\n",
      " |-- A_z_500c: double (nullable = true)\n",
      " |-- Acc_Log_Vmax_1_Tdyn: double (nullable = true)\n",
      " |-- Acc_Log_Vmax_Inst: double (nullable = true)\n",
      " |-- Acc_Rate_100Myr: double (nullable = true)\n",
      " |-- Acc_Rate_1_Tdyn: double (nullable = true)\n",
      " |-- Acc_Rate_2_Tdyn: double (nullable = true)\n",
      " |-- Acc_Rate_Inst: double (nullable = true)\n",
      " |-- Acc_Rate_Mpeak: double (nullable = true)\n",
      " |-- Acc_Scale: double (nullable = true)\n",
      " |-- Breadth_first_ID: long (nullable = true)\n",
      " |-- Depth_first_ID: long (nullable = true)\n",
      " |-- First_Acc_Mvir: double (nullable = true)\n",
      " |-- First_Acc_Scale: double (nullable = true)\n",
      " |-- First_Acc_Vmax: double (nullable = true)\n",
      " |-- Future_merger_MMP_ID: long (nullable = true)\n",
      " |-- Halfmass_Radius: double (nullable = true)\n",
      " |-- Halfmass_Scale: double (nullable = true)\n",
      " |-- Jx: double (nullable = true)\n",
      " |-- Jy: double (nullable = true)\n",
      " |-- Jz: double (nullable = true)\n",
      " |-- Last_mainleaf_depthfirst_ID: long (nullable = true)\n",
      " |-- Last_progenitor_depthfirst_ID: long (nullable = true)\n",
      " |-- Log_Vmax_Vmax_max_Tdyn_Tmpeak_: double (nullable = true)\n",
      " |-- M200b: double (nullable = true)\n",
      " |-- M200c: double (nullable = true)\n",
      " |-- M2500c: double (nullable = true)\n",
      " |-- M500c: double (nullable = true)\n",
      " |-- M_pe_Behroozi: double (nullable = true)\n",
      " |-- M_pe_Diemer: double (nullable = true)\n",
      " |-- Macc: double (nullable = true)\n",
      " |-- Mpeak: double (nullable = true)\n",
      " |-- Mpeak_Scale: double (nullable = true)\n",
      " |-- Mvir: double (nullable = true)\n",
      " |-- Mvir_all: double (nullable = true)\n",
      " |-- Next_coprogenitor_depthfirst_ID: long (nullable = true)\n",
      " |-- Orig_halo_ID: long (nullable = true)\n",
      " |-- Rs_Klypin: double (nullable = true)\n",
      " |-- Rvir: double (nullable = true)\n",
      " |-- Snap_idx: double (nullable = true)\n",
      " |-- Spin: double (nullable = true)\n",
      " |-- Spin_Bullock: double (nullable = true)\n",
      " |-- T_U: double (nullable = true)\n",
      " |-- Tidal_Force: double (nullable = true)\n",
      " |-- Tidal_Force_Tdyn: double (nullable = true)\n",
      " |-- Tidal_ID: long (nullable = true)\n",
      " |-- Time_to_future_merger: double (nullable = true)\n",
      " |-- Tree_root_ID: long (nullable = true)\n",
      " |-- Vacc: double (nullable = true)\n",
      " |-- Vmax_Mpeak: double (nullable = true)\n",
      " |-- Voff: double (nullable = true)\n",
      " |-- Vpeak: double (nullable = true)\n",
      " |-- Xoff: double (nullable = true)\n",
      " |-- b_to_a: double (nullable = true)\n",
      " |-- b_to_a_500c: double (nullable = true)\n",
      " |-- c_to_a: double (nullable = true)\n",
      " |-- c_to_a_500c: double (nullable = true)\n",
      " |-- desc_id: long (nullable = true)\n",
      " |-- desc_pid: long (nullable = true)\n",
      " |-- desc_scale: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- mmp: long (nullable = true)\n",
      " |-- num_prog: long (nullable = true)\n",
      " |-- phantom: long (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      " |-- rs: double (nullable = true)\n",
      " |-- rvmax: double (nullable = true)\n",
      " |-- sam_Mvir: double (nullable = true)\n",
      " |-- scale: double (nullable = true)\n",
      " |-- scale_of_last_MM: double (nullable = true)\n",
      " |-- upid: long (nullable = true)\n",
      " |-- vmax: double (nullable = true)\n",
      " |-- vrms: double (nullable = true)\n",
      " |-- vx: double (nullable = true)\n",
      " |-- vy: double (nullable = true)\n",
      " |-- vz: double (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- redshift: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_halos.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a summary of basic statistics about a DataFrame in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+\n",
      "|summary|Mvir                |Rvir              |CvirKly           |\n",
      "+-------+--------------------+------------------+------------------+\n",
      "|count  |4                   |4                 |4                 |\n",
      "|mean   |2.018E15            |2569.4049999999997|5.424931600046005 |\n",
      "|stddev |6.164414002968972E12|2.5905533514418204|0.7664338349708684|\n",
      "|min    |2.014E15            |2567.76           |4.371386643604742 |\n",
      "|25%    |2.014E15            |2567.76           |4.371386643604742 |\n",
      "|50%    |2.014E15            |2567.77           |5.387601362598065 |\n",
      "|75%    |2.017E15            |2568.88           |5.803175291640688 |\n",
      "|max    |2.027E15            |2573.21           |6.137563102340523 |\n",
      "+-------+--------------------+------------------+------------------+\n",
      "\n",
      "CPU times: user 5.23 ms, sys: 716 µs, total: 5.95 ms\n",
      "Wall time: 8.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hosts.select('Mvir', 'Rvir', 'CvirKly').summary().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not need the percentiles, instead of `summary` you can use `describe` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a summary of statistics about a DataFrame in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 ms, sys: 7 µs, total: 13.2 ms\n",
      "Wall time: 11.8 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mvir</th>\n",
       "      <th>Rvir</th>\n",
       "      <th>CvirKly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.018000e+15</td>\n",
       "      <td>2569.405000</td>\n",
       "      <td>5.424932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.164414e+12</td>\n",
       "      <td>2.590553</td>\n",
       "      <td>0.766434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.014000e+15</td>\n",
       "      <td>2567.760000</td>\n",
       "      <td>4.371387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.014000e+15</td>\n",
       "      <td>2567.767500</td>\n",
       "      <td>5.133548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.015500e+15</td>\n",
       "      <td>2568.325000</td>\n",
       "      <td>5.595388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.019500e+15</td>\n",
       "      <td>2569.962500</td>\n",
       "      <td>5.886772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.027000e+15</td>\n",
       "      <td>2573.210000</td>\n",
       "      <td>6.137563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Mvir         Rvir   CvirKly\n",
       "count  4.000000e+00     4.000000  4.000000\n",
       "mean   2.018000e+15  2569.405000  5.424932\n",
       "std    6.164414e+12     2.590553  0.766434\n",
       "min    2.014000e+15  2567.760000  4.371387\n",
       "25%    2.014000e+15  2567.767500  5.133548\n",
       "50%    2.015500e+15  2568.325000  5.595388\n",
       "75%    2.019500e+15  2569.962500  5.886772\n",
       "max    2.027000e+15  2573.210000  6.137563"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "hosts_pdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimize performance asking spark to cache the dataframe in memory if we are going to perform several operations on the same dataframe, but use it carefully because you can ran out of memory.\n",
    "\n",
    "We have serveral options:\n",
    "- halos_0.cache(): to keep it in memory\n",
    "- halos_0.persist(): to keep it in memory or disk\n",
    "\n",
    "And when done we have to run:\n",
    "- halos_0.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also convenient, when we do not know the name of the function to apply, to use the `F.xxx` alternative notation, in this way we can automplete available functions (using `Tab`) and show help (using `Shift+Tab`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(Mvir)|\n",
      "+------------------+\n",
      "|8.4807751816282E10|\n",
      "+------------------+\n",
      "\n",
      "CPU times: user 1.6 ms, sys: 2.75 ms, total: 4.36 ms\n",
      "Wall time: 5.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "halos_0.select(F.avg('Mvir')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Python list of hosts ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idhost = [row.id for row in distinct.select('id').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3814028194986, 101876720636316, 139500640393732, 214679739377433]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts = hosts.withColumn('CvirKly', expr('Rvir/Rs_Klypin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1: using spark approxQuantile relativeError 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The third argument of `approxQuantile` indicates the `relativeError` between 0 and 1. 0 means we want to compute the exact quantile, this is much more expensive computationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the median of `Mvir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.83 ms, sys: 3.4 ms, total: 8.23 ms\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quantiles = hosts.approxQuantile('Mvir', [0.5], 1)\n",
    "Mvir = quantiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.21 ms, sys: 462 µs, total: 6.67 ms\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quantiles = hosts.approxQuantile('Rvir', [0.5], 1)\n",
    "Rvir = quantiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.39 ms, sys: 1.85 ms, total: 9.24 ms\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quantiles = hosts.approxQuantile('CvirKly', [0.5], 1)\n",
    "CvirKly = quantiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties Mean host: Mvir median = 2014000000000000.0 Rvir = 2567.76 CvirKly = 4.371386643604742\n"
     ]
    }
   ],
   "source": [
    "print('Properties Mean host:','Mvir median =', Mvir, 'Rvir =', Rvir, 'CvirKly =', CvirKly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2: using spark approxQuantile full precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.54 ms, sys: 3.94 ms, total: 10.5 ms\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quantiles = hosts.approxQuantile('Mvir', [0.5], 0)\n",
    "Mvir = quantiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The third argument of `approxQuantile` indicates the `relativeError` between 0 and 1. 0 means we want to compute the exact quantile, this is much more expensive computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 ms, sys: 4.53 ms, total: 9.69 ms\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quantiles = hosts.approxQuantile('Rvir', [0.5], 0)\n",
    "Rvir = quantiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.96 ms, sys: 1.2 ms, total: 8.15 ms\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quantiles = hosts.approxQuantile('CvirKly', [0.5], 0)\n",
    "CvirKly = quantiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties Mean host: Mvir median = 2014000000000000.0 Rvir = 2567.77 CvirKly = 5.387601362598065\n"
     ]
    }
   ],
   "source": [
    "print('Properties Mean host:','Mvir median =', Mvir, 'Rvir =', Rvir, 'CvirKly =', CvirKly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 3: using python3 statitistics package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mvirs = [row.Mvir for row in hosts.select('Mvir').collect()]\n",
    "Rvirs = [row.Rvir for row in hosts.select('Rvir').collect()]\n",
    "CvirKlys = [row.CvirKly for row in hosts.select('CvirKly').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 3 µs, total: 16 µs\n",
      "Wall time: 19.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Mvir_median = median(Mvirs)\n",
    "Rvir_median = median(Rvirs)\n",
    "CvirKly_median = median(CvirKlys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties Mean host: Mvir median = 2015500000000000.0 Rvir = 2568.325 CvirKly = 5.595388327119377\n"
     ]
    }
   ],
   "source": [
    "print('Properties Mean host:','Mvir median =', Mvir_median, 'Rvir =', Rvir_median, 'CvirKly =', CvirKly_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 4: using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.36 ms, sys: 1.56 ms, total: 8.92 ms\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hosts_pdf = hosts.select('Mvir', 'Rvir', 'CvirKly').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 531 µs, sys: 0 ns, total: 531 µs\n",
      "Wall time: 468 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Mvir_median_pandas = hosts_pdf['Mvir'].median()\n",
    "Rvir_median_pandas = hosts_pdf['Rvir'].median()\n",
    "CvirKly_median_pandas = hosts_pdf['CvirKly'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties Mean host: Mvir median = 2015500000000000.0 Rvir = 2568.325 CvirKly = 5.595388327119377\n"
     ]
    }
   ],
   "source": [
    "print('Properties Mean host:','Mvir median =', Mvir_median_pandas, 'Rvir =', Rvir_median_pandas, 'CvirKly =', CvirKly_median_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "In general the programs are divided in:\n",
    "- Phase 1: extracting & transforming data: selecting data and adding additional columns with some computations. We can dump the selected data to HDFS in parquet format if we want to reuse it.\n",
    "- Phase 2: calculate some statistics in the extracted data\n",
    "- Phase 3: saving the results\n",
    "\n",
    "If we have custom python code we want to use we can parallelize the data processing using sc.parallelize()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to use programs with parameters\n",
    "There are ready to use programs for specific calculations, in that case you only have specify the required parameters for the program and then launch it using:\n",
    "\n",
    "    spark-submit compute-subhalo-clusters.py --z 0 --min_mass 3e10 --max_mass 10e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion optimizations applied when transforming from HDF5 to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original hdf5 files have been splitted into smaller files of 512MB so each one has one parquet row group.\n",
    "\n",
    "Optimizations performed:\n",
    "- The parquet row group size is 512MB\n",
    "- Each file has been uploaded to hdfs with a blocksize of 512MB.\n",
    "- The halos are `partitioned` by redshift\n",
    "\n",
    "The correspondence between the snapshot number and the redshift can be found here:\n",
    "[UCHUU Snapshot Redshift correspondences](http://www.skiesanduniverses.org/resources/Uchuu_snapshot_redshift_scalefactor.txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
