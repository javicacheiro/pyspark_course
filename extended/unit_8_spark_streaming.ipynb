{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 8 Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "8.1. Introduction to Stream Processing with Spark\n",
    "  8.1.1. Spark Streaming API (DStream)\n",
    "  8.1.2. Structured Streaming API\n",
    "  8.1.3. Stream Processing Model\n",
    "  8.1.4. Input Sources\n",
    "  8.1.5. Output Sinks\n",
    "  8.1.6. Output Mode\n",
    "  8.1.7. Fault Tolerance and Restarts\n",
    "  8.1.8. Typical structure of a Spark Streaming application\n",
    "  \n",
    "8.2. Windowing and Aggregates\n",
    "  8.2.1. Stateless vs Stateful Transformations\n",
    "  8.2.2. Event Time and Windowing    \n",
    "  8.2.3. Tumbling Window\n",
    "  8.2.4. Sliding Window\n",
    "  8.2.5. Watermarking\n",
    "  \n",
    "8.3. Joins\n",
    "  8.3.1. Stream-Static Joins\n",
    "  8.3.2. Stream-Stream Joins\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Introduction to Stream Processing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start reviewing how Spark operates in the standard batch processing mode:\n",
    "\n",
    "![Standard batch processing operation](https://bigdata.cesga.es/img/spark_streaming-non_streaming_operation.png)\n",
    "\n",
    "In batch mode, we have a input data source, we apply some transformations and we write the output to the given storage.\n",
    "\n",
    "When procesing streaming data source we have to introduce a new axis, **time**, because in this case the input source is constantly generating new input data as time evolves.\n",
    "\n",
    "![Microbatches](https://bigdata.cesga.es/img/spark_streaming-microbatch_generation.png)\n",
    "In stream processing mode Spark divides the input data stream in micro-batches and then each micro-batch is processed in a series of small jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.1. Spark Streaming API (DStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Streaming API, aka DStream, is the implementation of Spark Streaming based on RDDs. You can find it in legacy projects but for new projects the newer Structured Streaming API is recommended.\n",
    "\n",
    "NOTE: There are no longer updates to Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.2. Structured Streaming API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Structured Streaming API is the new streaming API that uses the Spark SQL engine, ie. the DataFrame API.\n",
    "\n",
    "Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "The idea behind both Spark Streaming and Structured Streaming is to divide the stream of data into **micro-batches** and each micro-batch its processed as a small job, achieving end-to-end latencies as low as 100 milliseconds.\n",
    "\n",
    "To achive lower latencies, there is also a low-latency processing mode called **Continuous Processing** which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees.\n",
    "\n",
    "The Spark SQL engine (Catalyst) takes care of running the series of jobs incrementally and continuously updating the final result as streaming data continues to arrive.\n",
    "\n",
    "The system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.3. Stream Processing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind spark structured streaming is to treat the live data stream as a table that is being continously appended.\n",
    "\n",
    "![Unbounded table](https://bigdata.cesga.es/img/spark_streaming-unbounded_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.4. Input Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Socket source (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rate source (for testing and benchmarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the number of partitions to simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .option(\"numPartitions\", 2) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"json\") \\\n",
    "  .option(\"path\", \"path/to/source/dir\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be: parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1,topic2\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose where to start in the stream with the `startingOffsets` option:\n",
    "```python\n",
    ".option(\"startingOffsets\", \"earliest\")\n",
    "```\n",
    "\n",
    "`earliest` will start from the earliest offsets of the topics partitions, `latest` will start from the latest offsets.\n",
    "\n",
    "We can also use a json string specifying a starting offset for each TopicPartition, in this case you can use `-2` to refer earliest and `-1` to refer to latest.\n",
    "```python\n",
    ".option(\"startingOffsets\", \"\"\"{\"topic1\": {\"0\": 100, \"1\": -2}, \"topic2\": {\"0\": -2}}\"\"\")\n",
    "```\n",
    "\n",
    "This only applies when a new query is started, resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 1: [Unit_8_structured_streaming-dataframe_schema.py](exercises/Unit_8_structured_streaming-dataframe_schema.py)\n",
    "- Review the code\n",
    "- Run the app using:\n",
    "```\n",
    "    spark-submit Unit_8_structured_streaming-dataframe_schema.py bigdata.cesga.es 80\n",
    "```    \n",
    "- What is the schema of the dataframe that is generated from the stream?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 2: [Unit_8_input_source_rate.py](exercises/Unit_8_input_source_rate.py)\n",
    "- Check how the rate input source works. This source is very useful for testing.\n",
    "- Run the app using:\n",
    "```\n",
    "    spark-submit Unit_8_input_source_rate.py\n",
    "```   \n",
    "- Experiment with the rowsPerSecond and numPartitions options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 3: [Unit_8_input_source_json_files.py](exercises/Unit_8_input_source_json_files.py)\n",
    "- Review the code\n",
    "- Create the news directory and upload news1.json file\n",
    "```\n",
    "hdfs dfs -mkdir news\n",
    "hdfs dfs -put news1.json news\n",
    "```\n",
    "- Run the app using\n",
    "```\n",
    "spark-submit Unit_8_input_source_json_files.py news\n",
    "```\n",
    "- Keep the app running and upload a new json file to the same directory\n",
    "```\n",
    "dfs dfs -put news2.json news\n",
    "```\n",
    "- Keep the app running and upload a final json file to the same directory\n",
    "```\n",
    "dfs dfs -put news3.json news\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.5. Output Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File sink: stores the output to a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"path/to/destination/dir\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka sink: stores the output to one or more topics in Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "    .option(\"topic\", \"orders\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataframe being written to Kafka should have the following columns in schema:\n",
    "- `key` (optional) of type string or binary\n",
    "- `value` (required) of type string or binary\n",
    "- `topic` (optional) of type string\n",
    "\n",
    "The `value column` is the only required option.\n",
    "\n",
    "The `topic column` is required if the `topic configuration option` is not specified. If a topic column exists then its value is used as the topic when writing the given row to Kafka. In case both the topic column and the topic option are both specified the topic configuration option overrides the topic column.\n",
    "\n",
    "If a `key column` is not specified then a `null` valued key column will be automatically added.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = products.selectExpr(\n",
    "    \"cast(order_id as string) as key\",\n",
    "    \"\"\"to_json(\n",
    "        named_struct(\n",
    "            'order_id', order_id,\n",
    "            'product_id', product_id,\n",
    "            'count', count\n",
    "        )\n",
    "    ) as value\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Console sink (for debugging): prints the output to stdout every time there is a trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `truncate` option allows to control if the output is truncated in case the content of a given cell is too long (`true` by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Memory sink (for debugging):  stores the output in the memory of the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"mytable\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then interactively query the \"mytable\" dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ForeachBatch: runs custom write logic on every micro-batch of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Custom function that transforms and writes df to storage\n",
    "    pass\n",
    "  \n",
    "df.writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foreach: runs custom write logic on every row of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    # Custom function that writes row to storage\n",
    "    pass\n",
    "    \n",
    "df.writeStream \\\n",
    "    .foreach(process_row) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.6. Output mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Append Mode*: only the new rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Update Mode*: only new and updated rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Complete Mode*: the entire updated result table will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete mode is only supported if there are streaming aggregations as it would be infeasible to keep all unaggregated data in the result dataframe.\n",
    "\n",
    "In complete mode all aggregate data has to be preserved so you must be careful on how much the result dataframe is growing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.7. Fault Tolerance and Restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream source operation modes:\n",
    "- **At most once**: a row of data is delivered to the application at most once. It could happen that is not delivered so data could be lost.\n",
    "- **At least once**: a row of data is delivered to the application at least once. No data is ever lost but it could be delivered several times, so data can be duplicated. \n",
    "- **Exactly once**: a row of data is guaranteed to be delivered exactly once. No data loss, no duplicated records.\n",
    "\n",
    "Restarts:\n",
    "- The ability to restart a streaming app from where it left\n",
    "\n",
    "**To achieve fault tolerance and restarts Spark depends on the underlying input source functionality.** The streaming source should support offsets (like Kafka) so the position in the stream can be tracked and set. Spark uses checkpointing and write ahead logs to record the offsets of the stream beaing processed in each trigger.\n",
    "\n",
    "The output sinks in Spark Structured Streaming are designed to be idempotent, so they can handle reprocessing correctly.\n",
    "\n",
    "To create a streaming app with Spark Structured Streaming that can achive **exactly-once** semantics we will have to:\n",
    "- Use a replayable source (like Kafka)\n",
    "- Use checkpointing\n",
    "- Use deterministic computation (same input -> same output, no randomness or processing time dependency)\n",
    "- Use idempotent sink (can handle duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.8. Typical structure of a Spark Streaming application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general spark streaming applications have the following structure:\n",
    "1. Read from the streaming source into a input dataframe (input source)\n",
    "2. Process the input dataframe and transform it in the output dataframe\n",
    "3. Write the output dataframe (output sink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's start with two labs to see how we can use the \"rate\" source and \"memory\" sink to work interactively:\n",
    "- Lab 4: [Unit_8_input_source_rate_output_memory.py](exercises/Unit_8_input_source_rate_output_memory.py) (non interactive: submit with \"spark-submit\")\n",
    "- Lab 5: [Unit_8_interactive_streaming.ipynb](exercises/Unit_8_interactive_streaming.ipynb) (interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a basic streaming app with the \"socket\" source so we can test the different output modes:\n",
    "- Lab 6: [Unit_8_structured_streaming_basics.ipynb](exercises/Unit_8_structured_streaming_basics.ipynb)\n",
    "\n",
    "Finally let's try to implement word count in a streaming app:\n",
    "- Lab 7: [Unit_8_socket_wordcount.ipynb](exercises/Unit_8_socket_wordcount.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Windowing and Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Stateless vs Stateful Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stateless transformations (complete output mode is not supported):\n",
    "- select\n",
    "- filter\n",
    "- map\n",
    "- flatMap\n",
    "- explode\n",
    "\n",
    "Stateful transformations (excessive state can lead to of memory errors):\n",
    "- Grouping\n",
    "- Aggregations\n",
    "- Windowing\n",
    "- Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Event Time and Windowing    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concepts:\n",
    "- Event time\n",
    "- Trigger time\n",
    "- Windowing: tumbling window, sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3. Tumbling Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tumbling Window](http://bigdata.cesga.es/img/spark_streaming_tumbling_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumbling_window = df.groupBy(\n",
    "    window(col(\"created_at\"), \"10 minutes\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4. Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sliding Window](http://bigdata.cesga.es/img/spark_streaming_sliding_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window = df.groupBy(\n",
    "    window(col(\"created_at\"), \"10 minutes\", \"5 minutes\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5. Watermarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watermarking allows spark to clean up the **state store** discarding old data. \n",
    "\n",
    "To define a watermark for a query we specify the event time column (it must be the same that we will use in the groupBy) and the threshold on how late the data is expected to be in terms of event time (late data outside the watermark will be discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_with_watermark = df \\\n",
    "    .withWatermark(\"created_at\", \"30 minutes\") \\\n",
    "    .groupBy(window(col(\"created_at\"), \"10 minutes\", \"5 minutes\")) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. Stream-Static Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df.join(static_df, expr(join_expr), join_type) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. Stream-Stream Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_stream = stream_1.join(stream_2, expr(join_expr), join_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting in local mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit --master local[3] --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 Unit_8_kafka.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a Column into pyspark.sql.types.TimestampType using the optionally specified format. Specify formats according to datetime pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         created_at|\n",
      "+-------------------+\n",
      "|2022-09-10 10:12:03|\n",
      "|2022-09-22 16:42:03|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(created_at=u'2022-09-10 10:12:03'),\n",
       " Row(created_at=u'2022-09-22 16:42:03')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(created_at='2022-09-10 10:12:03'), Row(created_at='2022-09-22 16:42:03')])\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 ts|\n",
      "+-------------------+\n",
      "|2022-09-10 10:12:03|\n",
      "|2022-09-22 16:42:03|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ts=datetime.datetime(2022, 9, 10, 10, 12, 3)),\n",
       " Row(ts=datetime.datetime(2022, 9, 22, 16, 42, 3))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "df2 = df.select(to_timestamp(col('created_at'), 'yyyy-MM-dd HH:mm:ss').alias('ts'))\n",
    "df2.show()\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `cast` method of a `Column`, but in this case we can not indicate the time format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ts=datetime.datetime(2022, 9, 10, 10, 12, 3)),\n",
       " Row(ts=datetime.datetime(2022, 9, 22, 16, 42, 3))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col('created_at').cast('timestamp').alias('ts')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses a column containing a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------+\n",
      "|key|value                                                                                                       |\n",
      "+---+------------------------------------------------------------------------------------------------------------+\n",
      "|1  |{\"order\": 1, \"products\": [{\"product\": \"P1\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C1\"}|\n",
      "|2  |{\"order\": 2, \"products\": [{\"product\": \"P3\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C2\"}|\n",
      "+---+------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(key=1, value=u'{\"order\": 1, \"products\": [{\"product\": \"P1\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C1\"}'),\n",
       " Row(key=2, value=u'{\"order\": 2, \"products\": [{\"product\": \"P3\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C2\"}')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(key=1, value='{\"order\": 1, \"products\": [{\"product\": \"P1\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C1\"}'),\n",
    "    Row(key=2, value='{\"order\": 2, \"products\": [{\"product\": \"P3\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C2\"}')\n",
    "])\n",
    "df.show(truncate=False)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|converted                  |\n",
      "+---------------------------+\n",
      "|[1, [[P1, 1], [P2, 1]], C1]|\n",
      "|[2, [[P3, 1], [P2, 1]], C2]|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(converted=Row(order=1, products=[Row(product=u'P1', amount=1), Row(product=u'P2', amount=1)], customer=u'C1')),\n",
       " Row(converted=Row(order=2, products=[Row(product=u'P3', amount=1), Row(product=u'P2', amount=1)], customer=u'C2'))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, IntegerType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order\", LongType()),\n",
    "    StructField(\"products\", \n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"product\", StringType()),\n",
    "                StructField(\"amount\", IntegerType())\n",
    "            ])\n",
    "        )\n",
    "    ),\n",
    "    StructField(\"customer\", StringType())\n",
    "])\n",
    "    \n",
    "               \n",
    "df2 = df.select(from_json(col('value').cast('string'), schema).alias('converted'))\n",
    "df2.show(truncate=False)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some quick rules of thump for schema definition:\n",
    "```\n",
    "    {} -> StructType\n",
    "    [] -> ArrayType\n",
    "    123 -> IntegerType, LongType\n",
    "    12.24 -> FloatType, DoubleType\n",
    "    text -> StringType\n",
    "    True -> BooleanType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to import the `types` module as an alias so then we have autocompletion to look for the types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a column containing a StructType, ArrayType or a MapType into a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+\n",
      "|key|value                   |\n",
      "+---+------------------------+\n",
      "|1  |[2022-09-10 16:00:01, 1]|\n",
      "|2  |[2022-09-10 16:00:01, 2]|\n",
      "+---+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(key=1, value=Row(created_at=u'2022-09-10 16:00:01', order_id=1)),\n",
       " Row(key=2, value=Row(created_at=u'2022-09-10 16:00:01', order_id=2))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(key=1, value=Row(created_at=\"2022-09-10 16:00:01\", order_id=1)),\n",
    "                            Row(key=2, value=Row(created_at=\"2022-09-10 16:00:01\", order_id=2))])\n",
    "df.show(truncate=False)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|json                                             |\n",
      "+-------------------------------------------------+\n",
      "|{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":1}|\n",
      "|{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":2}|\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(json=u'{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":1}'),\n",
       " Row(json=u'{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":2}')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df2 = df.select(to_json(col('value')).alias('json'))\n",
    "df2.show(truncate=False)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Streaming Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input Sources\n",
    "- Output Sinks\n",
    "- Output mode\n",
    "- Streaming Query\n",
    "- Schema\n",
    "- Triggers\n",
    "- Checkpointing\n",
    "- Fault tolerance and exactly once processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lab Unit_8_tumbling_window.ipynb\n",
    "- Lab Unit_8_sliding_window.ipynb\n",
    "- Lab Processing Orders:\n",
    "  - Unit_8_processing_orders_lab.py\n",
    "  - Unit_8_orders_producer_kafka-python.py\n",
    "- Lab Twitter\n",
    "  - Unit_8_twitter_developing_the_app.ipynb\n",
    "  - Unit_8_twitter_to_kafka.py\n",
    "  - Unit_8_twitter_sentiment_analysis.py\n",
    "  - Unit_8_twitter_sentiment_analysis_alternative_implementation_with_textblob.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming API (DStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Streaming API (DStream) is the implementation of Spark Streaming based on RDDs. It now longer receives updates but you can find it in existing projects.\n",
    "\n",
    "In this lab we will see how it has been used in a real-life use case to detect SSH brute-force attacks in real-time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lab Structured Streaming (DStream): Review the code of a production app using the legacy API\n",
    "  - Unit_8_ssh_attack_detector-dstream_app.py\n",
    "  - Unit_8_ssh_attack_detector-submit_script.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DStream: [Spark Streaming Programming Guide (legacy)](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Structured Streaming + Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "- [Real-Time Stream Processing Using Apache Spark 3 for Python Developers](https://www.packtpub.com/product/real-time-stream-processing-using-apache-spark-3-for-python-developers-video/9781803246543)\n",
    "- [Beginning Apache Spark 3: With DataFrame, Spark SQL, Structured Streaming, and Spark Machine Learning Library](https://www.amazon.es/Beginning-Apache-Spark-DataFrame-Structured/dp/1484273826)\n",
    "- [Spark in Action, Second Edition](https://www.manning.com/books/spark-in-action-second-edition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
