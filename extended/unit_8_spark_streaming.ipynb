{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 8 Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "```\n",
    "8.1. Introduction to Stream Processing with Spark\n",
    "  8.1.1. Spark Streaming API (DStream)\n",
    "  8.1.2. Structured Streaming API\n",
    "  8.1.3. Stream Processing Model\n",
    "  8.1.4. Input Sources\n",
    "  8.1.5. Output Sinks\n",
    "  8.1.6. Output Mode\n",
    "  8.1.7. Fault Tolerance and Restarts\n",
    "  8.1.8. Typical structure of a Spark Streaming application\n",
    "  \n",
    "8.2. Windowing and Aggregates\n",
    "  8.2.1. Stateless vs Stateful Transformations\n",
    "  8.2.2. Event Time and Windowing    \n",
    "  8.2.3. Tumbling Window\n",
    "  8.2.4. Sliding Window\n",
    "  8.2.5. Watermarking\n",
    "  \n",
    "8.3. Joins\n",
    "  8.3.1. Stream-Static Joins\n",
    "  8.3.2. Stream-Stream Joins\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Introduction to Stream Processing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start reviewing how Spark operates in the standard batch processing mode:\n",
    "\n",
    "![Standard batch processing operation](https://bigdata.cesga.es/img/spark_streaming-non_streaming_operation.png)\n",
    "\n",
    "In batch mode, we have a input data source, we apply some transformations and we write the output to the given storage.\n",
    "\n",
    "When procesing streaming data source we have to introduce a new axis, **time**, because in this case the input source is constantly generating new input data as time evolves.\n",
    "\n",
    "![Microbatches](https://bigdata.cesga.es/img/spark_streaming-microbatch_generation.png)\n",
    "In stream processing mode Spark divides the input data stream in micro-batches and then each micro-batch is processed in a series of small jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.1. Spark Streaming API (DStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Streaming API, aka DStream, is the implementation of Spark Streaming based on RDDs. You can find it in legacy projects but for new projects the newer Structured Streaming API is recommended.\n",
    "\n",
    "NOTE: There are no longer updates to Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.2. Structured Streaming API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Structured Streaming API is the new streaming API that uses the Spark SQL engine, ie. the DataFrame API.\n",
    "\n",
    "Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "The idea behind both Spark Streaming and Structured Streaming is to divide the stream of data into **micro-batches** and each micro-batch its processed as a small job, achieving end-to-end latencies as low as 100 milliseconds.\n",
    "\n",
    "To achive lower latencies, there is also a low-latency processing mode called **Continuous Processing** which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees.\n",
    "\n",
    "The Spark SQL engine (Catalyst) takes care of running the series of jobs incrementally and continuously updating the final result as streaming data continues to arrive.\n",
    "\n",
    "The system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.3. Stream Processing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea behind spark structured streaming is to treat the live data stream as a table that is being continously appended.\n",
    "\n",
    "![Unbounded table](https://bigdata.cesga.es/img/spark_streaming-unbounded_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.4. Input Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Socket source (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rate source (for testing and benchmarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the number of partitions to simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .option(\"numPartitions\", 2) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"json\") \\\n",
    "  .option(\"path\", \"path/to/source/dir\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be: parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose where to start in the stream with the `startingOffsets` option:\n",
    "```\n",
    ".option(\"startingOffsets\", \"earliest\")\n",
    "```\n",
    "\n",
    "The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab: Unit_8_structured_streaming-dataframe_schema.py\n",
    "- Review the code\n",
    "- Run the app using:\n",
    "```\n",
    "    spark-submit Unit_8_structured_streaming-dataframe_schema.py bigdata.cesga.es 80\n",
    "```    \n",
    "- What is the schema of the dataframe that is generated from the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab: Unit_8_input_source_rate.py\n",
    "- Check how the rate input source works. This source is very useful for testing.\n",
    "- Run the app using:\n",
    "```\n",
    "    spark-submit Unit_8_input_source_rate.py\n",
    "```   \n",
    "- Experiment with the rowsPerSecond and numPartitions options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.5. Output Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File sink: stores the output to a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"path/to/destination/dir\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format can be parquet, json, csv, orc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka sink: stores the output to one or more topics in Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "    .option(\"subscribe\", \"orders\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataframe being written to Kafka should have the following columns in schema:\n",
    "- `key` (optional) of type string or binary\n",
    "- `value` (required) of type string or binary\n",
    "- `topic` (optional) of type string\n",
    "\n",
    "The topic column is required if the “topic” configuration option is not specified.\n",
    "\n",
    "The value column is the only required option. If a key column is not specified then a null valued key column will be automatically added.  If a topic column exists then its value is used as the topic when writing the given row to Kafka, unless the “topic” configuration option is set i.e., the “topic” configuration option overrides the topic column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = products.selectExpr(\n",
    "    \"cast(order_id as string) as key\",\n",
    "    \"\"\"to_json(\n",
    "        named_struct(\n",
    "            'order_id', order_id,\n",
    "            'product_id', product_id,\n",
    "            'count', count\n",
    "        )\n",
    "    ) as value\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Console sink (for debugging): prints the output to stdout every time there is a trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Memory sink (for debugging):  stores the output in the memory of the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"mytable\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then interactively query the \"mytable\" dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ForeachBatch: runs custom write logic on every micro-batch of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Custom function that transforms and writes df to storage\n",
    "    pass\n",
    "  \n",
    "df.writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foreach sink: runs custom write logic on every row of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    # Custom function that writes row to storage\n",
    "    pass\n",
    "    \n",
    "df.writeStream \\\n",
    "    .foreach(process_row) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.6. Output mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Complete Mode*: the entire updated result table will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Append Mode*: only the new rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Update Mode*: only the new and updated rows will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.7. Fault Tolerance and Restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream source operation modes:\n",
    "- At most once: a row of data is delivered to the application at most once. It could happen that is not delivered so data could be lost.\n",
    "- At least once: a row of data is delivered to the application at least once. No data is ever lost but it could be delivered several times, so data can be duplicated. \n",
    "- Exactly once: a row of data is guaranteed to be delivered exactly once. No lost of data, no duplicated records.\n",
    "\n",
    "Restarts:\n",
    "- The ability to restart a streaming app from where it left\n",
    "\n",
    "To achieve fault tolerance and restarts Spark depends on the underlying input source functionality. The streaming source should support offsets (like Kafka) so the position in the stream can be tracked and set. Spark uses checkpointing and write ahead logs to record the offsets of the stream beaing processed in each trigger.\n",
    "\n",
    "The output sinks in Spark Structured Streaming are designed to be idempotent, so they can handle reprocessing correctly.\n",
    "\n",
    "So to create a streaming app in Structured Spark Streaming that can achive **exactly-once** semantics we will have to:\n",
    "- Use a replayable source (like Kafka)\n",
    "- Use checkpointing\n",
    "- Use deterministic computation (same input -> same output, no randomness or processing time dependency)\n",
    "- Use idempotent sink (can handle duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.8. Typical structure of a Spark Streaming application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general spark streaming applications have the following structure:\n",
    "1. Read from the streaming source into a input dataframe (input source)\n",
    "2. Process the input dataframe and transform it in the output dataframe\n",
    "3. Write the output dataframe (output sink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's start with two labs to see how we can use the \"rate\" source and \"memory\" sink to work interactively:\n",
    "- Lab Unit_8_input_source_rate_output_memory.py (non interactive: submit with \"spark-submit\")\n",
    "- Lab Unit_8_interactive_streaming.ipynb (interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a basic stream app with the \"socket\" source so we can test the different output modes:\n",
    "- Lab Unit_8_structured_streaming_basics\n",
    "\n",
    "Finally let's try to implement word count in a streaming app:\n",
    "- Lab Unit_8_socket_wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Windowing and Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Stateless vs Stateful Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stateless transformations (complete output mode is not supported):\n",
    "- select\n",
    "- filter\n",
    "- map\n",
    "- flatMap\n",
    "- explode\n",
    "\n",
    "Stateful transformations (excessive state can lead to of memory errors):\n",
    "- Grouping\n",
    "- Aggregations\n",
    "- Windowing\n",
    "- Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Event Time and Windowing    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concepts:\n",
    "- Event time\n",
    "- Trigger time\n",
    "- Windowing: tumbling window, sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3. Tumbling Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tumbling Window](http://bigdata.cesga.es/img/spark_streaming_tumbling_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumbling_window = df.groupBy(\n",
    "    window(col(\"created_at\"), \"10 minutes\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.4. Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sliding Window](http://bigdata.cesga.es/img/spark_streaming_sliding_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window = df.groupBy(\n",
    "    window(col(\"created_at\"), \"10 minutes\", \"5 minutes\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.5. Watermarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watermarking allows spark to clean up the **state store** discarding old data. \n",
    "\n",
    "To define a watermark for a query we specify the event time column (it must be the same that we will use in the groupBy) and the threshold on how late the data is expected to be in terms of event time (late data outside the watermark will be discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_with_watermark = df \\\n",
    "    .withWatermark(\"created_at\", \"30 minutes\") \\\n",
    "    .groupBy(window(col(\"created_at\"), \"10 minutes\", \"5 minutes\")) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. Stream-Static Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df.join(static_df, expr(join_expr), join_type) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. Stream-Stream Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_stream = stream_1.join(stream_2, expr(join_expr), join_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting in local mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit --master local[3] --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 Unit_8_kafka.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a Column into pyspark.sql.types.TimestampType using the optionally specified format. Specify formats according to datetime pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         created_at|\n",
      "+-------------------+\n",
      "|2022-09-10 10:12:03|\n",
      "|2022-09-22 16:42:03|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(created_at=u'2022-09-10 10:12:03'),\n",
       " Row(created_at=u'2022-09-22 16:42:03')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(created_at='2022-09-10 10:12:03'), Row(created_at='2022-09-22 16:42:03')])\n",
    "df.show()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 ts|\n",
      "+-------------------+\n",
      "|2022-09-10 10:12:03|\n",
      "|2022-09-22 16:42:03|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ts=datetime.datetime(2022, 9, 10, 10, 12, 3)),\n",
       " Row(ts=datetime.datetime(2022, 9, 22, 16, 42, 3))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "df2 = df.select(to_timestamp(col('created_at'), 'yyyy-MM-dd HH:mm:ss').alias('ts'))\n",
    "df2.show()\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `cast` method of a `Column`, but in this case we can not indicate the time format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ts=datetime.datetime(2022, 9, 10, 10, 12, 3)),\n",
       " Row(ts=datetime.datetime(2022, 9, 22, 16, 42, 3))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col('created_at').cast('timestamp').alias('ts')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses a column containing a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------+\n",
      "|key|value                                                                                                       |\n",
      "+---+------------------------------------------------------------------------------------------------------------+\n",
      "|1  |{\"order\": 1, \"products\": [{\"product\": \"P1\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C1\"}|\n",
      "|2  |{\"order\": 2, \"products\": [{\"product\": \"P3\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C2\"}|\n",
      "+---+------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(key=1, value=u'{\"order\": 1, \"products\": [{\"product\": \"P1\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C1\"}'),\n",
       " Row(key=2, value=u'{\"order\": 2, \"products\": [{\"product\": \"P3\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C2\"}')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(key=1, value='{\"order\": 1, \"products\": [{\"product\": \"P1\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C1\"}'),\n",
    "    Row(key=2, value='{\"order\": 2, \"products\": [{\"product\": \"P3\", \"amount\": 1}, {\"product\": \"P2\", \"amount\": 1}], \"customer\": \"C2\"}')\n",
    "])\n",
    "df.show(truncate=False)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|converted                  |\n",
      "+---------------------------+\n",
      "|[1, [[P1, 1], [P2, 1]], C1]|\n",
      "|[2, [[P3, 1], [P2, 1]], C2]|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(converted=Row(order=1, products=[Row(product=u'P1', amount=1), Row(product=u'P2', amount=1)], customer=u'C1')),\n",
       " Row(converted=Row(order=2, products=[Row(product=u'P3', amount=1), Row(product=u'P2', amount=1)], customer=u'C2'))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, IntegerType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order\", LongType()),\n",
    "    StructField(\"products\", \n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"product\", StringType()),\n",
    "                StructField(\"amount\", IntegerType())\n",
    "            ])\n",
    "        )\n",
    "    ),\n",
    "    StructField(\"customer\", StringType())\n",
    "])\n",
    "    \n",
    "               \n",
    "df2 = df.select(from_json(col('value').cast('string'), schema).alias('converted'))\n",
    "df2.show(truncate=False)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some quick rules of thump for schema definition:\n",
    "```\n",
    "    {} -> StructType\n",
    "    [] -> ArrayType\n",
    "    123 -> IntegerType, LongType\n",
    "    12.24 -> FloatType, DoubleType\n",
    "    text -> StringType\n",
    "    True -> BooleanType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to import the `types` module as an alias so then we have autocompletion to look for the types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a column containing a StructType, ArrayType or a MapType into a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+\n",
      "|key|value                   |\n",
      "+---+------------------------+\n",
      "|1  |[2022-09-10 16:00:01, 1]|\n",
      "|2  |[2022-09-10 16:00:01, 2]|\n",
      "+---+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(key=1, value=Row(created_at=u'2022-09-10 16:00:01', order_id=1)),\n",
       " Row(key=2, value=Row(created_at=u'2022-09-10 16:00:01', order_id=2))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(key=1, value=Row(created_at=\"2022-09-10 16:00:01\", order_id=1)),\n",
    "                            Row(key=2, value=Row(created_at=\"2022-09-10 16:00:01\", order_id=2))])\n",
    "df.show(truncate=False)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|json                                             |\n",
      "+-------------------------------------------------+\n",
      "|{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":1}|\n",
      "|{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":2}|\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(json=u'{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":1}'),\n",
       " Row(json=u'{\"created_at\":\"2022-09-10 16:00:01\",\"order_id\":2}')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df2 = df.select(to_json(col('value')).alias('json'))\n",
    "df2.show(truncate=False)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Streaming Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input Sources\n",
    "- Output Sinks\n",
    "- Output mode\n",
    "- Streaming Query\n",
    "- Schema\n",
    "- Triggers\n",
    "- Checkpointing\n",
    "- Fault tolerance and exactly once processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lab Unit_8_tumbling_window.ipynb\n",
    "- Lab Unit_8_sliding_window.ipynb\n",
    "- Lab Interactive Streaming\n",
    "- Lab Processing Orders:\n",
    "  - Unit_8_processing_orders_lab.py\n",
    "  - Unit_8_orders_producer_kafka-python.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming API (DStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Streaming API (DStream) is the implementation of Spark Streaming based on RDDs. It now longer receives updates but you can find it in existing projects.\n",
    "\n",
    "In this lab we will see how it has been used in a real-life use case to detect SSH brute-force attacks in real-time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lab Structured Streaming (DStream): Review the code of a production app using the legacy API\n",
    "  - Unit_8_ssh_attack_detector-dstream_app.py\n",
    "  - Unit_8_ssh_attack_detector-submit_script.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DStream: [Spark Streaming Programming Guide (legacy)](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Structured Streaming + Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "- [Real-Time Stream Processing Using Apache Spark 3 for Python Developers](https://www.packtpub.com/product/real-time-stream-processing-using-apache-spark-3-for-python-developers-video/9781803246543)\n",
    "- [Beginning Apache Spark 3: With DataFrame, Spark SQL, Structured Streaming, and Spark Machine Learning Library](https://www.amazon.es/Beginning-Apache-Spark-DataFrame-Structured/dp/1484273826)\n",
    "- [Spark in Action, Second Edition](https://www.manning.com/books/spark-in-action-second-edition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
