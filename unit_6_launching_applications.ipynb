{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 6 Launching Spark Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "** Creating a Spark application **\n",
    "\n",
    "** Submitting an application to YARN **\n",
    "\n",
    "** Simple application submission example **\n",
    "\n",
    "** Cluster vs Client mode **\n",
    "\n",
    "** Adding dependencies **\n",
    "\n",
    "** Complex application submission example **\n",
    "\n",
    "** How-to install additional Python packages **\n",
    "\n",
    "** Using additional compression codec libraries: eg. LZO **\n",
    "\n",
    "** Sending the application in the background **\n",
    "\n",
    "** Dynamic resource allocation **\n",
    "\n",
    "** Hadoop Component Versions **\n",
    "\n",
    "** Overriding configuration directory **\n",
    "\n",
    "** Run an interactive shell **\n",
    "\n",
    "** Exercises **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark application\n",
    "An application is very similar to a notebook, but there are some minor changes that must be applied.\n",
    "\n",
    "The interactive notebook creates automatically the SparkContext (sc) so in a standard application you must take care of creating it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sc = SparkContext(appName='My Application')\n",
    "    # ...\n",
    "    # Application specific code\n",
    "    # ..\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of using Spark SQL and DataFrames, additionaly you need to create an sqlContext instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting an application to YARN\n",
    "\n",
    "To submit an application to YARN you use the **spark-submit** utility:\n",
    "\n",
    "```\n",
    "spark-submit\n",
    "  --name NAME                 A name of your application.\n",
    "\n",
    "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.\n",
    "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
    "                              on one of the worker machines inside the cluster (\"cluster\")\n",
    "                              (Default: client).\n",
    "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
    "\n",
    "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
    "  \n",
    "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
    "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
    "                              (Default: 1).\n",
    "                              \n",
    "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
    "  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n",
    "                              or all available cores on the worker in standalone mode)\n",
    "\n",
    "  --proxy-user NAME           User to impersonate when submitting the application.\n",
    "```\n",
    "\n",
    "The main options to take into account for resource allocation are:\n",
    "\n",
    "* The `--num-executors` (spark.executor.instances as configuration property) option controls how many executors it will allocate for the application on the cluster .\n",
    "* The `--executor-memory` (spark.executor.memory configuration property) option controls the memory allocated per executor.\n",
    "* The `--executor-cores` (spark.executor.cores configuration property) option controls the cores allocated per executor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple application submission example\n",
    "\n",
    "    spark-submit --master yarn --name testWC test.py\n",
    "    spark-submit --master yarn --deploy-mode cluster --name testWC test.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster vs Client mode\n",
    "\n",
    "![Cluster mode](https://image.slidesharecdn.com/th-1150a-hall1-feng-v2-140617142634-phpapp01/95/sparkonyarn-empower-spark-applications-on-hadoop-cluster-9-638.jpg?cb=1403015417)\n",
    "\n",
    "![Client mode](https://image.slidesharecdn.com/th-1150a-hall1-feng-v2-140617142634-phpapp01/95/sparkonyarn-empower-spark-applications-on-hadoop-cluster-10-638.jpg?cb=1403015417)\n",
    "\n",
    "Image Source: [Spark-on-YARN: Empower Spark Applications on Hadoop Cluster](https://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dependencies\n",
    "\n",
    "To add dependencies you have two options:\n",
    "- The **--jars** option transfers associated jar files to the cluster.\n",
    "- The **--packages** option pulls directly from Spark packages. This approach requires an internet connection.\n",
    "- The **--py-files** option adds .zip, .egg, or .py files to the PYTHONPATH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dependencies: packages\n",
    "\n",
    "    spark-submit --packages com.databricks:spark-avro_2.10:2.0.1 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dependencies: jar files\n",
    "\n",
    "    spark-submit --jars /jar_path/spark-streaming-kafka-assembly_2.10-1.6.1.2.4.2.0-258.jar ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dependencies: egg files\n",
    "In case you have an egg file of the package you want to use, you use the `--py-files` option of spark-submit or the `sc.addPyFile()` method to make it available to the application. After that you can make use of it in your application in the standard way.\n",
    "\n",
    "    spark-submit --py-files /egg_path/avro-1.8.1-py2.7.egg ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we add the egg file to the application environment\n",
    "sc.addPyFile('/home/cesga/jlopez/packages/ClusterShell-1.7.3-py2.7.egg')\n",
    "# Then we can import and use it in the standard way\n",
    "from ClusterShell.NodeSet import NodeSet\n",
    "nodeset = NodeSet('c[6601-6610]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex application submission example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see a real example of how to submit a real application that consumes data from Kafka in avro format using Spark Streaming:\n",
    "\n",
    "```\n",
    "spark-submit --master yarn --deploy-mode cluster \\\n",
    "             --num-executors 2 \\\n",
    "             --conf spark.yarn.submit.waitAppCompletion=false  \\\n",
    "             --packages com.databricks:spark-avro_2.10:2.0.1 \\\n",
    "             --jars /home/cesga/jlopez/packages/spark-streaming-kafka-assembly_2.10-1.6.1.2.4.2.0-258.jar \\\n",
    "             --py-files /home/cesga/jlopez/packages/avro-1.8.1-py2.7.egg \\\n",
    "             --name 'SSH attack detector' \\             \n",
    "             ssh_attack_detector.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How-to install additional Python packages\n",
    "The simplest way is to use **pip** with the `--user` option:\n",
    "\n",
    "    pip install --user pymongo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using additional compression codec libraries: eg. LZO\n",
    "If you try to use a codec library without specifying where the codec resides, you will see an error.\n",
    "For example, if the hadoop-lzo codec file cannot be found during spark-submit, Spark will generate the following message:\n",
    "\n",
    "    Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.\n",
    "\n",
    "To solve it specify the hadoop-lzo jar file with the --jars option in your job submit command. \n",
    "\n",
    "    spark-submit --driver-memory 1G --executor-memory 1G --master yarn-client --jars /usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar test_read_write.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the application in the background\n",
    "\n",
    "By default when you submit an application the spark-submit command keeps active waiting for application output. To avoid this behaviour use spark.yarn.submit.waitAppCompletion=false:\n",
    "\n",
    "    spark-submit --conf spark.yarn.submit.waitAppCompletion=false ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic resource allocation\n",
    "Spark provides a mechanism to dynamically adjust the resources your application occupies based on the workload. This means that your application may give resources back to the cluster if they are no longer used and request them again later when there is demand. This feature is particularly useful if multiple applications share resources in your Spark cluster.\n",
    "\n",
    "CESGA HDP cluster has this feature enabled so it **automatically expands new executors when they are needed**, instead of fixing them at launch time with --num-executors.\n",
    "\n",
    "This allows that interactive jobs dynamically add and remove executors during execution.\n",
    "\n",
    "When you specify the `--num-executors` option dynamic resource allocation is disabled automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Component Versions\n",
    "\n",
    "The cluster has HDP 2.4.2 with Hadoop 2.7.1\n",
    "\n",
    "![HDP 2.4.2 component versions](http://hortonworks.com/wp-content/uploads/2016/03/asparagus-hdp25.jpg)\n",
    "\n",
    "You can also check the versions with:\n",
    "\n",
    "    hdp-select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding configuration directory\n",
    "To specify a different configuration directory other than the default “SPARK_HOME/conf”, you can set SPARK_CONF_DIR. Spark will use the configuration files (spark-defaults.conf, spark-env.sh, log4j.properties, etc) from this directory.\n",
    "\n",
    "Example:\n",
    "\n",
    "    export SPARK_CONF_DIR=/home/cesga/jlopez/conf/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an interactive shell\n",
    "Additionally to jupyter notebooks you can also use a command line interactive shell:\n",
    "\n",
    "    pyspark --master yarn --num-executors 4 --executor-cores 6\n",
    "\n",
    "    --num-executors NUM         Number of executors to launch (Default: 2).\n",
    "    --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode)\n",
    "    --driver-cores NUM          Number of cores used by the driver, only in cluster mode (Default: 1).\n",
    "    --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
    "    --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
    "    --proxy-user NAME           User to impersonate when submitting the application.\n",
    "\n",
    "\n",
    "To use ipython instead of python for an interactive session use:\n",
    "\n",
    "    PYSPARK_DRIVER_PYTHON=ipython pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Exercise: Modify the \"Unit 4 Working with meteorological data 2\" notebook and submit it to YARN\n",
    "* Exercise: Modify the \"Unit 5 Working with meteorological data\" notebook and submit it to YARN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
